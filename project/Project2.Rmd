---
title: "Project 2: Modeling, Testing, and Predicting"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#0:Data Set

For the purpose of this project, I used an official dataset used on Medicare.gov for hospital quality comparison which showed hospital ratings. This data set was obtained from Kaggle.com and the Centers for Medicare and Medicaid. This data set contained 8 different variables, including: Hospital Name (Categorical), State (Categorical), Zip Code (numerical), Hospital Type; Acute, Critical, or Children's Hosp. (Categorical), Ownership of Hospital;Voluntary Non Profit, Proprietary, or Government Owned (Categorical), Emergency Services Available; Yes=1/No=2 (Binary Numerical), Hospital Rating; Scale of 1-5 (Numerical), and Average Amount of Nights Spent by Patients (Numerical). Overall, these variables allowed me to analyze statistical trends.


#1:MANOVA/ANOVA Testing 

```{r}

library(AER)

datap<- read.csv("dataset.csv", 
                  header = TRUE, 
                   quote="\"", 
                  stringsAsFactors= TRUE, 
                 strip.white = TRUE)
data.frame(datap)
man1<-manova(cbind(Rating,Nights)~ZIP, data=datap)
summary(man1)
summary.aov(man1)


pairwise.t.test(datap$Rating,datap$Nights, p.adj="none")

pairwise.t.test(datap$ER,datap$Rating, p.adj="none")

library(rstatix)
group<- datap$ER
DVs<- datap %>% select(Rating,Nights)
sapply(split(DVs,group), mshapiro_test)

```
For this section, I performed five different statistical tests which included a MANOVA, two ANOVAs, and two t-tests. From the first MANOVA test, I was not able to obtain statistically significant differences between the hospitals. The Pillai value = 0.044111, the F value = 0.85371, and p value = 0.4341. There were two additional univariate ANOVA tests performed in order to determine the mean differences between the two data sets. For the Hospital Rating ANOVA test, I obtained an F value = 0.8962, and P-value = 0.3498. Additionally, I also ran a univariate ANOVA test on the 'Nights' variable, which showed the average amount of nights a patient spent at each respective hospitals. The statistics obtained for this test included, F value = 1.1661 and P-value= 0.287. The pairwise comparison tests, also known as t-tests, were conducted in order to determine the difference in values. The alpha value was adjusted with the Bonferroni method in order to avoid a Type I error. The p-value for Hospital Rating was < 0.05, indicating that the main assumptions are actually violated.


#2: Randomization test
```{r}
library(vegan)
library(plyr)
ddply(datap, "Rating", plyr::summarize, mean_Nights = mean(datap$Nights))

datap %>% group_by(Rating) %>% summarize(means = mean(Nights)) %>% summarize(mean_diff = diff(means))


x1<- datap$Rating
y1<- datap$Nights
x2<- datap$ZIP

cor(x1,y1, method= "pearson")

library(vegan)
dists <- datap %>% select(Rating, Nights, ZIP) %>% dist
adonis(dists~ER, data=datap)

#Observed F

library(tidyverse)
SST<- sum(dists^2)/200
SSW<- datap %>%group_by(datap$Rating)%>%select(Rating, Nights, ZIP, ER) %>%
do(d=dist(.[-1],"euclidean"))%>%ungroup()%>% summarize(sum(d[[1]]^2)/92 + sum(d[[2]]^2)/108)
F_obs<-((SST-SSW)/1)/(SSW/198) #observed F statistic
#Fs<-replicate(1000,
#new <- datap %>%mutate(Rating=sample(Rating))

#PERMUTATION
#SSW <- new%>% group_by(Rating) %>% select(Rating,Nights, ZIP, ER) %>%
#do(d=dist(.[-1],"euclidean")) %>% ungroup() %>%
#summarize(sum(d[[1]]^2)/92 + sum(d[[2]]^2)/108)
#((SST-SSW)/1)/(SSW/198)
#Would not let me knit :(


```
I was able to employ a Pearson correlation test in order to determine a relationship between Hospital Rating and amount of Nights spent, on average, by a patient. The correlation value = 0.1998, compared to  HO:Rating will not have a strong correlation to average amount of nights spent by patients in the hospital, ZIP code, or if the hospital provides ER Services. HA: Rating will have a strong correlation to the average amount of nights spent by patients in the hospital, ZIP code, or if the hospital provides ER services.



#3: Linear Regression Model
```{r}
library(ggplot2)
datap$Rating<- datap$Rating - mean(datap$Rating)
datap$Nights<- datap$Nights - mean(datap$Nights)

fit1<- lm(Nights ~ ER*Rating, data= datap)
summary(fit1)

coef(fit1)
datap %>% ggplot(aes(Rating,Nights))+geom_point()+geom_smooth(method = 'lm', se=F)

cor(datap$Rating, datap$Nights)

res<- fit1$residuals
fitval<- fit1$fitted.values

ggplot()+geom_point(aes(fitval,res))+geom_hline(yintercept = 0, color = 'blue')



ggplot()+geom_histogram(aes(res))
ggplot()+geom_qq(aes(sample=res))+geom_qq()

coeftest(fit1)[,1:2]
coeftest(fit1, vcov=vcovHC(fit1))[,1:2]


fit<- lm(Rating~Nights, data=datap)
SST<- sum((datap$Rating-mean(datap$Rating))^2)
SSR<- sum((fit$fitted.values-mean(datap$Rating))^2)
SSE<- sum(fit$residuals^2)

SSR/SST
```






#4: Regression Model with bootstrapped std error
```{r}

library("dplyr")

samp_ditn<- replicate(5000, {
  boot_dat<-boot_dat<-datap[sample(nrow(datap),replace = TRUE),]
  fit<- lm(datap$Rating~ datap$Nights* datap$ZIP, data= boot_dat)
  coef(fit)
  })

# SE Estimate

samp_ditn%>%t%>%as.data.frame%>%summarize_all(sd)

samp_ditn %>% t %>% as.data.frame %>% pivot_longer(1:3) %>% group_by(datap$Hospital) %>% summarize(lower=quantile(value,.025), upper=quantile(value,.975))



```

Interestingly enough, after performing statistical tests the results showed equal SE values. 


#5: Logistic Regression Model
```{r}

library(tidyverse)
library(lmtest)
library(plotROC)
data1<- datap%>%mutate(y=ifelse(ER=="Yes",0,1))
head(data1)

fit2<-glm(y~ ER,data=data1,family=binomial(link="logit"))
coeftest(fit2)
exp(coef(fit2))

logistic<-function(x){exp(x)/(1+exp(x))}

#Confusion Matrix
table(truth=datap$ER, prediction=datap$Rating>1)%>%addmargins

#Accuracy
(15+5)/40

#TPR Sensitivity
15/18

#TNR Specificity
5/22

#PPV Precision
15/32

#AUC
widths<-diff(datap$y)
heights<-vector()
#for(i in 1:100) heights[i]<-datap$y[i]+datap$y[i+1]
AUC<-sum(heights*widths/2)
AUC%>%round(3)


#density plot
datap$logit<-predict(fit2,type="link")
datap%>%ggplot()+geom_density(aes(logit,color=Rating,fill=Rating), alpha=.4)+ theme(legend.position=c(.3,.6))+geom_vline(xintercept=2)+xlab("logit (log-odds)")+geom_rug(aes(logit,color=Rating))


#ROC
library(plotROC)
ROCplot<-ggplot(datap)+geom_roc(aes(d=ER,m=Rating), n.cuts=0)
ROCplot

calc_auc(ROCplot)

```

From the analysis performed above, I was able to obtain a accuracy value of 0.5, TPR value = 0.8333, TNR value = 0.2273, PPV value = 0.4688, and AUC value extremely low (~0). The low AUC value indicates that the model is not reliable or extremely accurate in this specific case.


#6: 2nd Logistic Regression
```{r}

#fit model
library(tidyverse)
library(lmtest)
library(pROC)
library(glmnet)
class_diag<-function(probs,truth){
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1 
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n<- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}





```

